import streamlit as st
from scholar_search import run_scholar_search
from search_api import search_openalex,  search_arxiv, search_crossref
import pandas as pd
import json
import os
import glob
from dotenv import load_dotenv
from utils import filter_duplicates, save_results_to_json, save_results_to_database,get_latest_json,convert_latest_json_to_gsheet,enrich_with_firecrawl, summarize_filtered_papers, filter_top_papers


# ===================== PAGE CONFIG =====================
st.set_page_config(page_title="Paper Search App", layout="wide")

# -------------------- CONFIG FILE --------------------
RESULTS_DIR = "results"
DATABASE_DIR = "database"
DATABASE_FILE = "papers_db.json"
ENV_PATH = ".env"

if not os.path.exists(RESULTS_DIR):
    os.makedirs(RESULTS_DIR)

if not os.path.exists(ENV_PATH):
    open(ENV_PATH, "a").close()

load_dotenv(ENV_PATH)


# ===================== TABS =====================
tab1, tab2 = st.tabs([
    "üåê All APIs + Scholar",
    "üìÅ Danh s√°ch k·∫øt qu·∫£"
])

# ===================== TAB 1 =====================
with tab1:
    st.subheader("üîπ T√¨m ki·∫øm tr√™n All APIs + Google Scholar")

    # Nh·∫≠p t·ª´ kh√≥a v√† s·ªë l∆∞·ª£ng b√†i m·ªôt l·∫ßn
    keyword_tab1 = st.text_input("Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm (All APIs + Scholar):", key="keyword_tab1")
    max_results_tab1 = st.number_input("S·ªë l∆∞·ª£ng b√†i mu·ªën l·∫•y m·ªói ngu·ªìn", min_value=1, max_value=200, value=10, key="max_results_tab1")

    if st.button("üîç T√¨m ki·∫øm All APIs + Scholar"):
        if not keyword_tab1.strip():
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm!")
        else:
            with st.spinner("ƒêang t√¨m ki·∫øm tr√™n t·∫•t c·∫£ c√°c API..."):
                # 1. G·ªçi c√°c API
                openalex_res = search_openalex(query=keyword_tab1, rows=max_results_tab1)
                arxiv_res = search_arxiv(query=keyword_tab1, rows=max_results_tab1)
                crossref_res = search_crossref(query=keyword_tab1, rows=max_results_tab1)

                # Google Scholar
                scholar_data = run_scholar_search(keyword_tab1, max_results_tab1)

                # 2. H·ª£p nh·∫•t k·∫øt qu·∫£
                merged_results = []
                for res in [openalex_res, arxiv_res, crossref_res, scholar_data]:
                    merged_results.extend(res)

                # 3. L·ªçc tr√πng 
                st.info("‚è≥ ƒêang l·ªçc b√†i b√°o tr√πng...")
                unique_results = filter_duplicates(merged_results)

                # 4. Crawl abstract b·ªï sung b·∫±ng Firecrawl
                st.info("‚è≥ ƒêang b·ªï sung abstract...")
                enriched_results = enrich_with_firecrawl(unique_results)

                # 5. L·ªçc b√†i kh√¥ng li√™n quan
                st.info("‚è≥ ƒêang l·ªçc b√†i b√°o...")
                top_results = filter_top_papers(enriched_results)

                # 6. T√≥m t·∫Øt abstract
                st.info("‚è≥ ƒêang t√≥m t·∫Øt abstract...")
                summarized_results = summarize_filtered_papers(top_results)

                # 7. L∆∞u k·∫øt qu·∫£
                saved_file = save_results_to_json(
                    summarized_results,
                    output_dir=RESULTS_DIR,
                    prefix=f"allapi_scholar_{keyword_tab1.replace(' ', '_')}"
                )
                if saved_file:
                    save_results_to_database(saved_file)
                    st.success(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ enriched v√†o: {saved_file}")
                convert_latest_json_to_gsheet()


                # 8. Hi·ªÉn th·ªã k·∫øt qu·∫£
                latest_file = get_latest_json()
                if latest_file:
                    try:
                        with open(latest_file, "r", encoding="utf-8") as f:
                            today_results = json.load(f)
                        df = pd.DataFrame(today_results)
                        st.subheader("üìÑ K·∫øt qu·∫£ b√†i b√°o h√¥m nay")
                        st.dataframe(df)
                    except Exception as e:
                        st.error(f"‚ùå L·ªói khi ƒë·ªçc file JSON: {e}")
                else:
                    st.info("‚ÑπÔ∏è Ch∆∞a c√≥ file k·∫øt qu·∫£ h√¥m nay.")
                    
                # 9. N√∫t t·∫£i file JSON
                st.download_button(
                    label="üì• T·∫£i k·∫øt qu·∫£ JSON",
                    data=json.dumps(today_results, indent=2, ensure_ascii=False),
                    file_name=os.path.basename(saved_file),
                    mime="application/json"
                )




# ===================== TAB 2 =====================
with tab2:
    st.subheader("üìÇ Danh s√°ch t·∫•t c·∫£ file k·∫øt qu·∫£ ƒë√£ l∆∞u")

    files = sorted(glob.glob(os.path.join(RESULTS_DIR, "*.json")), key=os.path.getmtime, reverse=True)

    if not files:
        st.info("‚ö†Ô∏è Ch∆∞a c√≥ file k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c l∆∞u.")
    else:
        for file_path in files:
            filename = os.path.basename(file_path)
            file_size = os.path.getsize(file_path) / 1024  # KB
            st.write(f"**üìÑ {filename}** ({file_size:.2f} KB)")

            with open(file_path, "rb") as f:
                st.download_button(
                    label=f"üì• T·∫£i {filename}",
                    data=f.read(),
                    file_name=filename,
                    mime="application/json",
                    key=filename
                )
            st.markdown("---")
