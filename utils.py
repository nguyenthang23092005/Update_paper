import os
import glob
import json
import time
import requests
import re
from dateutil import parser
from datetime import datetime, timedelta
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from gspread_formatting import (
    CellFormat, Color, TextFormat, format_cell_range, set_column_width
)
from googleapiclient.discovery import build
from dotenv import load_dotenv
from google.genai import Client
from google.genai.types import GenerateContentConfig
load_dotenv()
FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

client = Client(api_key=GOOGLE_API_KEY)



RESULTS_DIR = "results"
DATABASE_DIR = "database"
DATABASE_FILE = "papers_db.json"
SPREADSHEET_ID = "1snMFj6e4X3YUK_48xXJlb8VhLwcS4vSxb69LgoBcDO4"
creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")


def get_creds():
    scopes = ["https://www.googleapis.com/auth/spreadsheets"]

    # D√†nh cho GitHub Actions
    creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if creds_path and os.path.exists(creds_path):
        return Credentials.from_service_account_file(creds_path, scopes=scopes)

    # D√†nh cho ch·∫°y local
    local_path = r"D:\GitHub\Key_gg_sheet\eternal-dynamo-474316-f6-382e31e4ae72.json"
    if os.path.exists(local_path):
        return Credentials.from_service_account_file(local_path, scopes=scopes)

    raise FileNotFoundError("‚ùå Kh√¥ng t√¨m th·∫•y file Google credential n√†o h·ª£p l·ªá.")

def normalize_key(paper):
    """
    Chu·∫©n h√≥a key ƒë·ªÉ so s√°nh tr√πng l·∫∑p:
    - ∆Øu ti√™n DOI (lowercase, b·ªè kho·∫£ng tr·∫Øng).
    - N·∫øu kh√¥ng c√≥ DOI ‚Üí d√πng link.
    - N·∫øu kh√¥ng c√≥ link ‚Üí d√πng title.
    """
    doi = (paper.get("doi") or "").strip().lower()
    link = (paper.get("link") or "").strip().lower()
    title = (paper.get("title") or "").strip().lower()

    if doi:
        return doi
    elif link:
        return link
    elif title:
        return title
    return ""

# ==============================
# L·∫•y file JSON m·ªõi nh·∫•t
# ==============================
def get_latest_json():
    """
    L·∫•y file JSON m·ªõi nh·∫•t theo ng√†y c√≥ d·∫°ng: YYYY-MM-DD_allapi_scholar_ndt.json
    """
    pattern = os.path.join(RESULTS_DIR, "*_allapi_scholar_*.json")
    json_files = glob.glob(pattern)

    if not json_files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON n√†o trong th∆∞ m·ª•c results/")
        return None

    # L·∫•y ng√†y t·ª´ t√™n file v√† ch·ªçn ng√†y m·ªõi nh·∫•t
    files_with_dates = []
    for f in json_files:
        base = os.path.basename(f)
        try:
            date_part = base.split("_")[0]  # L·∫•y ph·∫ßn YYYY-MM-DD
            datetime.strptime(date_part, "%Y-%m-%d")  # ki·ªÉm tra format
            files_with_dates.append((date_part, f))
        except Exception:
            continue

    if not files_with_dates:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON h·ª£p l·ªá theo ng√†y")
        return None

    # Ch·ªçn file c√≥ ng√†y m·ªõi nh·∫•t
    latest_file = max(files_with_dates, key=lambda x: x[0])[1]
    print(f"üìÇ File JSON m·ªõi nh·∫•t theo ng√†y: {latest_file}")
    return latest_file


# ==============================
# L∆∞u file JSON v·ªõi timestamp
# ==============================
def save_results_to_json(data, output_dir=RESULTS_DIR, prefix="allapi_scholar_ndt"):
    """
    L∆∞u k·∫øt qu·∫£ v√†o file JSON v·ªõi t√™n ch·ª©a timestamp.
    N·∫øu c√πng 1 ng√†y ƒë√£ c√≥ file -> load d·ªØ li·ªáu c≈©, merge th√™m d·ªØ li·ªáu m·ªõi (l·ªçc tr√πng), r·ªìi ghi ƒë√® l·∫°i.
    """
    os.makedirs(output_dir, exist_ok=True)
    today_str = datetime.now().strftime("%Y-%m-%d")

    # T√¨m file trong ng√†y h√¥m nay
    existing_file = None
    for fname in os.listdir(output_dir):
        if fname.startswith(today_str) and fname.endswith(f"{prefix}.json"):
            existing_file = os.path.join(output_dir, fname)
            break

    merged_data = []
    if existing_file:
        try:
            with open(existing_file, "r", encoding="utf-8") as f:
                old_data = json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file c≈© {existing_file}: {e}")
            old_data = []
        merged_data = old_data
    else:
        # N·∫øu ch∆∞a c√≥ file -> t·∫°o file m·ªõi
        timestamp = datetime.now().strftime("%Y-%m-%d")
        filename = f"{timestamp}_{prefix}.json"
        existing_file = os.path.join(output_dir, filename)

    # Merge d·ªØ li·ªáu (l·ªçc tr√πng theo key chu·∫©n h√≥a)
    existing_keys = {normalize_key(item) for item in merged_data if normalize_key(item)}
    new_filtered = [p for p in data if normalize_key(p) not in existing_keys]

    if not new_filtered:
        print("‚è© Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ th√™m.")
        return existing_file

    merged_data.extend(new_filtered)

    try:
        with open(existing_file, "w", encoding="utf-8") as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        print(f"üíæ ƒê√£ c·∫≠p nh·∫≠t file: {existing_file} (th√™m {len(new_filtered)} b√†i b√°o)")
        return existing_file
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u file JSON: {e}")
        return None


# ==============================
# Load Database DOI
# ==============================
def load_database(db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    os.makedirs(db_dir, exist_ok=True)
    db_path = os.path.join(db_dir, db_file)

    if os.path.exists(db_path):
        try:
            if os.path.getsize(db_path) == 0:
                return []
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è File JSON b·ªã l·ªói ho·∫∑c r·ªóng: {db_path}, t·∫°o database m·ªõi")
            return []
    else:
        return []



def save_database(data, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    os.makedirs(db_dir, exist_ok=True)
    db_path = os.path.join(db_dir, db_file)

    with open(db_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"üíæ Database ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t: {db_path}")

# ==============================
# C·∫≠p nh·∫≠t Database (l∆∞u Title + DOI)
# ==============================
def save_results_to_database(result_file, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    """
    ƒê·ªçc k·∫øt qu·∫£ t·ª´ file JSON v√† l∆∞u v√†o database.
    Chu·∫©n h√≥a key (doi/link/title) v√† lo·∫°i b·ªè tr√πng l·∫∑p.
    """
    if not os.path.exists(result_file):
        print(f"‚ùå File k·∫øt qu·∫£ kh√¥ng t·ªìn t·∫°i: {result_file}")
        return False

    try:
        with open(result_file, "r", encoding="utf-8") as f:
            results = json.load(f)
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc file k·∫øt qu·∫£ {result_file}: {e}")
        return False

    db_data = load_database(db_dir, db_file)
    db_dict = {normalize_key(item): item for item in db_data if normalize_key(item)}

    new_count = 0
    for paper in results:
        key = normalize_key(paper)
        if key and key not in db_dict:
            db_dict[key] = {
                "title": paper.get("title", "Untitled"),
                "doi": paper.get("doi", paper.get("link", ""))
            }
            new_count += 1

    save_database(list(db_dict.values()), db_dir, db_file)
    print(f"‚úÖ ƒê√£ th√™m {new_count} b√†i b√°o m·ªõi v√†o database t·ª´ {result_file}")
    return True



# ==============================
# L·ªçc b√†i b√°o tr√πng 
# ==============================
def filter_duplicates(new_results, results_dir=RESULTS_DIR, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    """
    L·ªçc tr√πng c√°c b√†i b√°o m·ªõi b·∫±ng key chu·∫©n h√≥a (doi/link/title).
    - N·∫øu file m·ªõi nh·∫•t l√† h√¥m nay ‚Üí kh√¥ng l·ªçc.
    - N·∫øu file m·ªõi nh·∫•t l√† h√¥m qua ‚Üí l·ªçc theo h√¥m qua.
    - N·∫øu kh√¥ng ph·∫£i h√¥m nay v√† kh√¥ng ph·∫£i h√¥m qua ‚Üí l·ªçc theo database.
    """
    today_str = datetime.now().strftime("%Y-%m-%d")
    yesterday_str = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

    # üîπ L·∫•y file JSON m·ªõi nh·∫•t
    latest_file = get_latest_json()
    if not latest_file:
        return new_results

    # üîπ ƒê·ªçc d·ªØ li·ªáu file m·ªõi nh·∫•t
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            old_results = json.load(f)
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc file {latest_file}: {e}")
        return new_results

    old_dates = {paper.get("pub_date", "") for paper in old_results}

    # ‚úÖ File h√¥m nay ‚Üí kh√¥ng l·ªçc
    if today_str in old_dates:
        print("‚è© File m·ªõi nh·∫•t ƒë√£ l√† h√¥m nay -> Kh√¥ng l·ªçc tr√πng.")
        return new_results

    # ‚úÖ Kh√¥ng ph·∫£i h√¥m nay ‚Üí l·ªçc
    # N·∫øu l√† h√¥m qua ‚Üí l·ªçc theo h√¥m qua
    if yesterday_str in old_dates:
        old_keys = {normalize_key(p) for p in old_results if normalize_key(p)}
        filtered_results = [p for p in new_results if normalize_key(p) not in old_keys]
        removed_count = len(new_results) - len(filtered_results)
        print(f"üóëÔ∏è ƒê√£ lo·∫°i b·ªè {removed_count} b√†i b√°o tr√πng v·ªõi h√¥m qua.")
        return filtered_results

    # ‚úÖ Kh√¥ng ph·∫£i h√¥m qua ‚Üí l·ªçc theo database
    db_path = os.path.join(db_dir, db_file)
    if not os.path.exists(db_path):
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y database -> Tr·∫£ v·ªÅ to√†n b·ªô d·ªØ li·ªáu m·ªõi.")
        return new_results

    try:
        with open(db_path, "r", encoding="utf-8") as f:
            db_data = json.load(f)
            db_keys = {normalize_key(item) for item in db_data if normalize_key(item)}
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc database {db_path}: {e}")
        return new_results

    filtered_results = [p for p in new_results if normalize_key(p) not in db_keys]
    removed_count = len(new_results) - len(filtered_results)
    print(f"üóëÔ∏è ƒê√£ lo·∫°i b·ªè {removed_count} b√†i b√°o tr√πng v·ªõi database.")
    return filtered_results


def tidy_up_sheet_auto(spreadsheet_id, sheet_name=None):
    # 1. K·∫øt n·ªëi Google Sheets API
    creds = get_creds()
    service = build("sheets", "v4", credentials=creds)

    # 2. L·∫•y metadata sheet
    sheet_metadata = service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()
    sheets = sheet_metadata.get("sheets", [])

    if not sheets:
        print("‚ö†Ô∏è File Google Sheets tr·ªëng, kh√¥ng c√≥ sheet n√†o.")
        return

    # 3. X√°c ƒë·ªãnh sheet_id
    sheet_id = None
    if sheet_name:
        for s in sheets:
            if s["properties"]["title"] == sheet_name:
                sheet_id = s["properties"]["sheetId"]
                break
    if not sheet_id:
        sheet_id = sheets[0]["properties"]["sheetId"]
        sheet_name = sheets[0]["properties"]["title"]
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y sheet b·∫°n ch·ªâ ƒë·ªãnh. S·ª≠ d·ª•ng sheet ƒë·∫ßu ti√™n: '{sheet_name}'")

    # 4. L·∫•y d·ªØ li·ªáu hi·ªán t·∫°i ƒë·ªÉ x√°c ƒë·ªãnh s·ªë h√†ng v√† c·ªôt
    result = service.spreadsheets().values().get(
        spreadsheetId=spreadsheet_id, range=sheet_name
    ).execute()
    values = result.get("values", [])
    num_rows = len(values) if values else 1
    num_cols = max(len(row) for row in values) if values else 1

    # 5. T√≠nh chi·ªÅu r·ªông c·ªôt d·ª±a tr√™n n·ªôi dung
    column_widths = [0] * num_cols
    for row in values:
        for col_index in range(num_cols):
            cell_value = row[col_index] if col_index < len(row) else ""
            column_widths[col_index] = max(column_widths[col_index], len(str(cell_value)))

    # 6. T·∫°o danh s√°ch requests
    requests = [
        # CƒÉn gi·ªØa header
        {
            "repeatCell": {
                "range": {
                    "sheetId": sheet_id,
                    "startRowIndex": 0,
                    "endRowIndex": 1
                },
                "cell": {
                    "userEnteredFormat": {
                        "horizontalAlignment": "CENTER",
                        "textFormat": {"bold": True}
                    }
                },
                "fields": "userEnteredFormat(textFormat,horizontalAlignment)"
            }
        },
        # B·∫≠t wrap text to√†n sheet
        {
            "repeatCell": {
                "range": {"sheetId": sheet_id},
                "cell": {"userEnteredFormat": {"wrapStrategy": "WRAP"}},
                "fields": "userEnteredFormat.wrapStrategy"
            }
        },
        # Bo vi·ªÅn to√†n b·∫£ng
        {
            "updateBorders": {
                "range": {
                    "sheetId": sheet_id,
                    "startRowIndex": 0,
                    "endRowIndex": num_rows,
                    "startColumnIndex": 0,
                    "endColumnIndex": num_cols
                },
                "top": {"style": "SOLID", "width": 1},
                "bottom": {"style": "SOLID", "width": 1},
                "left": {"style": "SOLID", "width": 1},
                "right": {"style": "SOLID", "width": 1},
                "innerHorizontal": {"style": "SOLID", "width": 1},
                "innerVertical": {"style": "SOLID", "width": 1}
            }
        }
    ]

    # 7. Resize c·ªôt d·ª±a tr√™n n·ªôi dung
    for col_index, max_len in enumerate(column_widths):
        width_pixels = max(80, min(max_len * 10, 400))  # min 80px, max 400px
        requests.append({
            "updateDimensionProperties": {
                "range": {
                    "sheetId": sheet_id,
                    "dimension": "COLUMNS",
                    "startIndex": col_index,
                    "endIndex": col_index + 1
                },
                "properties": {"pixelSize": width_pixels},
                "fields": "pixelSize"
            }
        })

    # 8. ƒê·∫∑t chi·ªÅu cao t·∫•t c·∫£ h√†ng ~10cm (10cm ‚âà 378px)
    requests.append({
        "updateDimensionProperties": {
            "range": {
                "sheetId": sheet_id,
                "dimension": "ROWS",
                "startIndex": 0,
                "endIndex": num_rows
            },
            "properties": {"pixelSize": 378},
            "fields": "pixelSize"
        }
    })

    # 9. G·ª≠i batchUpdate
    body = {"requests": requests}
    service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=body).execute()

    print(f"‚úÖ ƒê√£ cƒÉn ch·ªânh v√† ƒë·ªãnh d·∫°ng sheet '{sheet_name}' th√†nh c√¥ng!")



def append_json_to_gsheet(df, date_str):
    """Th√™m ho·∫∑c ghi ƒë√® d·ªØ li·ªáu JSON v√†o Google Sheet, kh√¥ng ƒë√® sang ng√†y kh√°c"""
    creds = get_creds()
    client = gspread.authorize(creds)
    sheet = client.open_by_key(SPREADSHEET_ID).sheet1

    all_values = sheet.get_all_values()
    last_col = chr(ord('A') + len(df.columns) - 1)

    # T√¨m xem ng√†y ƒë√£ t·ªìn t·∫°i ch∆∞a
    existing_row = None
    for idx, row in enumerate(all_values):
        if row and row[0].startswith(f"üìÖ Ng√†y {date_str}"):
            existing_row = idx + 1  # gspread index t·ª´ 1
            break

    if existing_row:
        print(f"‚ÑπÔ∏è Ng√†y {date_str} ƒë√£ t·ªìn t·∫°i tr√™n sheet. Ghi ƒë√® d·ªØ li·ªáu.")
        start_row = existing_row

        # T√¨m row cu·ªëi c√πng c·ªßa d·ªØ li·ªáu ng√†y h√¥m nay
        end_row = start_row + 1
        while end_row <= len(all_values):
            if all_values[end_row-1] and all_values[end_row-1][0].startswith("üìÖ Ng√†y"):
                break
            end_row += 1

        # X√≥a to√†n b·ªô v√πng d·ªØ li·ªáu c≈©
        sheet.delete_rows(start_row, end_row - 1)

        next_row = start_row
    else:
        next_row = len(all_values) + 3 if len(all_values) > 0 else 1  # th√™m 2 d√≤ng tr·ªëng

    # üóìÔ∏è Ghi ti√™u ƒë·ªÅ ng√†y
    sheet.update_cell(next_row, 1, f"üìÖ Ng√†y {date_str}")
    try:
        sheet.merge_cells(next_row, 1, next_row, len(df.columns))
    except Exception:
        pass

    # Format ti√™u ƒë·ªÅ ng√†y
    try:
        day_fmt = CellFormat(
            textFormat=TextFormat(bold=True, foregroundColor=Color(1, 0, 0), fontSize=13),
            horizontalAlignment='CENTER',
            backgroundColor=Color(0.85, 0.93, 1.0)
        )
        format_cell_range(sheet, f"A{next_row}:{last_col}{next_row}", day_fmt)
    except Exception:
        pass

    # üß© Ghi header v√† d·ªØ li·ªáu
    data_to_insert = [df.columns.values.tolist()] + df.values.tolist()
    sheet.update(f"A{next_row + 1}", data_to_insert)

    # Format header
    try:
        header_fmt = CellFormat(
            textFormat=TextFormat(bold=True),
            horizontalAlignment='CENTER',
            backgroundColor=Color(0.95, 0.95, 0.95)
        )
        format_cell_range(sheet, f"A{next_row + 1}:{last_col}{next_row + 1}", header_fmt)
    except Exception:
        pass

    # CƒÉn gi·ªØa d·ªØ li·ªáu
    try:
        data_end_row = next_row + len(df) + 1
        center_fmt = CellFormat(
            textFormat=TextFormat(bold=False),
            horizontalAlignment='CENTER'
        )
        format_cell_range(sheet, f"A{next_row + 2}:{last_col}{data_end_row}", center_fmt)
    except Exception:
        pass

    # Auto-width cho c√°c c·ªôt
    try:
        for i in range(1, len(df.columns) + 1):
            set_column_width(sheet, i, 160)
    except Exception:
        pass

    print(f"‚úÖ ƒê√£ th√™m/ghi ƒë√® d·ªØ li·ªáu ng√†y {date_str} v√†o Google Sheet")


def convert_latest_json_to_gsheet():
    """ƒê·ªçc file JSON m·ªõi nh·∫•t (h√¥m nay) v√† n·ªëi v√†o sheet"""
    latest_file = get_latest_json()
    if not latest_file:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON.")
        return

    today_str = datetime.now().strftime("%Y-%m-%d")
    base = os.path.basename(latest_file)
    file_date = base.split("_")[0]

    if file_date != today_str:
        print("‚ÑπÔ∏è File JSON m·ªõi nh·∫•t kh√¥ng ph·∫£i c·ªßa h√¥m nay.")
        return

    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    df = pd.DataFrame(data)
    append_json_to_gsheet(df, today_str)
    tidy_up_sheet_auto(SPREADSHEET_ID)
    
# ========================
# Merge & Save to One File
# ========================
def merge_and_save(all_results, filename):
    """
    G·ªôp t·∫•t c·∫£ k·∫øt qu·∫£ v√†o 1 file v√† lo·∫°i b·ªè tr√πng l·∫∑p.
    Tr√πng l·∫∑p ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi: title + authors ho·∫∑c link.
    """
    unique = {}
    for paper in all_results:
        key = (paper['title'].lower().strip(), paper['authors'].lower().strip(), paper['link'].lower().strip())
        if key not in unique:
            unique[key] = paper

    final_results = list(unique.values())

    if not os.path.exists(RESULTS_DIR):
        os.makedirs(RESULTS_DIR)

    filepath = os.path.join(RESULTS_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)

    print(f"Saved {len(final_results)} unique papers to {filepath}")

def fetch_abstract_and_pubdate_firecrawl(url):
    """
    D√πng Firecrawl Scrape API, tr√≠ch xu·∫•t to√†n b·ªô abstract v√† pubdate.
    """
    if not FIRECRAWL_API_KEY:
        raise ValueError("Thi·∫øu FIRECRAWL_API_KEY, h√£y set trong bi·∫øn m√¥i tr∆∞·ªùng.")

    api_url = "https://api.firecrawl.dev/v1/scrape"
    headers = {"Authorization": f"Bearer {FIRECRAWL_API_KEY}"}
    payload = {
        "url": url,
        "formats": ["markdown"],  
    }

    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=60)
        resp.raise_for_status()
        data = resp.json()
    except requests.exceptions.RequestException as e:
        print(f"[Firecrawl Error] {e}")
        return {"abstract": "Not Available", "pubdate": "Not Available"}

    content = data.get("data", {}).get("markdown", "")
    if not content:
        return {"abstract": "Not Available", "pubdate": "Not Available"}

    lines = content.splitlines()
    abstract_lines = []
    capture = False

    for line in lines:
        low = line.lower().strip()
        # B·∫Øt ƒë·∫ßu t·ª´ Abstract / T√≥m t·∫Øt
        if "abstract" in low or "t√≥m t·∫Øt" in low:
            capture = True
            continue
        # N·∫øu g·∫∑p Keywords / Introduction th√¨ d·ª´ng l·∫°i
        if capture and ("keywords" in low or "introduction" in low or "references" in low):
            break
        if capture:
            abstract_lines.append(line.strip())

    abstract = " ".join(abstract_lines).strip()
    
    # --- Tr√≠ch xu·∫•t pubdate t·ª´ markdown ---
    pubdate = "Not Available"
    # t√¨m c√°c m·∫´u nh∆∞ "Published: 2025-01-20" ho·∫∑c "Ng√†y xu·∫•t b·∫£n: 20 Jan 2025"
    date_patterns = [
        r'published[:\s]+(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
        r'published[:\s]+(\d{1,2}\s\w+\s\d{4})', # DD Month YYYY
        r'ng√†y xu·∫•t b·∫£n[:\s]+(\d{1,2}\s\w+\s\d{4})',
    ]
    for pattern in date_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            try:
                pubdate = str(parser.parse(match.group(1)).date())
                break
            except:
                continue
    return {"abstract": abstract, "pubdate": pubdate}


def enrich_with_firecrawl(results):
    """
    Nh·∫≠n danh s√°ch results (c√°c b√†i b√°o ƒë√£ crawl t·ª´ OpenAlex, Arxiv, etc.)
    N·∫øu abstract = 'Not Available' ho·∫∑c pubdate = 'Not Available' th√¨ d√πng Firecrawl l·∫•y.
    """
    for paper in results:
        needs_fetch = (
            (not paper.get("abstract") or paper["abstract"] == "Not Available") or
            (not paper.get("pubdate") or paper["pubdate"] == "Not Available")
        )
        if needs_fetch and paper.get("link") != "Not Available":
            print(f"Fetching abstract & pubdate with Firecrawl for: {paper['title']}")
            data = fetch_abstract_and_pubdate_firecrawl(paper["link"])
            paper["abstract"] = data["abstract"]
            paper["pubdate"] = data["pubdate"]
            time.sleep(6)  # tr√°nh b·ªã rate-limit
    return results


def evaluate_paper_combined(abstract, keywords):
    """
    Ki·ªÉm tra li√™n quan v√† ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng b√†i b√°o trong 1 b∆∞·ªõc.

    Returns:
        dict: {
            "related": bool,
            "score": int (0-10)
        }
    """
    prompt = f"""
    You are an expert in scientific paper evaluation.

    Task: 
    1. Determine if the following abstract is related to the topic: {", ".join(keywords)}. Answer only YES or NO.
    2. If it is related, rate its quality on a scale from 0 (very poor) to 10 (excellent). 
       If it is not related, give a score of 0.
    Only respond in the format: YES/NO and score as an integer (e.g., YES 7 or NO 0).

    Abstract:
    {abstract}
    """

    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=GenerateContentConfig(temperature=0)
        )
        text = response.text.strip().upper()
        related = "YES" in text
        score_match = re.search(r'\d+', text)
        score = int(score_match.group()) if score_match else 0
        score = min(max(score, 0), 10)
        return {"related": related, "score": score}
    except Exception as e:
        print(f"[Gemini Error - Combined Evaluation] {e}")
        return {"related": False, "score": 0}


# =========================================
# H√†m l·ªçc b√†i b√°o kh√¥ng c√≥ abstract ho·∫∑c kh√¥ng li√™n quan
# =========================================
def filter_top_papers(results, keywords=["Non-Destructive Testing"], top_n=10):
    """
    L·ªçc c√°c b√†i b√°o li√™n quan v√† ch·ªçn ra top N b√†i b√°o hay nh·∫•t d·ª±a tr√™n score AI.

    Parameters:
        results (list): Danh s√°ch b√†i b√°o, m·ªói b√†i b√°o l√† dict v·ªõi 'abstract' v√† 'title'.
        keywords (list): Danh s√°ch t·ª´ kh√≥a li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ nghi√™n c·ª©u.
        top_n (int): S·ªë b√†i b√°o mu·ªën gi·ªØ l·∫°i (m·∫∑c ƒë·ªãnh 10).

    Returns:
        list: Danh s√°ch b√†i b√°o ƒë√£ l·ªçc v√† s·∫Øp x·∫øp theo ch·∫•t l∆∞·ª£ng.
    """
    scored_papers = []

    for paper in results:
        title = paper.get("title", "Untitled")
        abstract = paper.get("abstract", "").strip()

        if not abstract or abstract.lower() == "not available":
            continue

        print(f"Checking relevance and quality for: {title}")
        # G·ªçi h√†m ƒë√°nh gi√° k·∫øt h·ª£p li√™n quan + ƒëi·ªÉm ch·∫•t l∆∞·ª£ng
        evaluation = evaluate_paper_combined(abstract, keywords)

        if evaluation["related"]:
            paper["score"] = evaluation["score"]
            scored_papers.append(paper)
        else:
            print(f"‚ùå Paper '{title}' is not relevant.")

        time.sleep(16)  # tr√°nh b·ªã rate-limit Gemini API

    # S·∫Øp x·∫øp theo score gi·∫£m d·∫ßn v√† ch·ªâ l·∫•y top N
    top_papers = sorted(scored_papers, key=lambda x: x["score"], reverse=True)[:top_n]
    return top_papers


# =========================================
# H√†m t√≥m t·∫Øt abstract
# =========================================
def summarize_with_genai(abstract):
    """
    D√πng Gemini API ƒë·ªÉ t√≥m t·∫Øt abstract th√†nh 3-4 c√¢u.

    Parameters:
        abstract (str): Abstract c·ªßa b√†i b√°o.

    Returns:
        str: T√≥m t·∫Øt abstract.
    """
    prompt = f"""
    Summarize the following abstract in 3-4 concise sentences.
    Use simple and clear academic English.

    Abstract:
    {abstract}
    """

    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",  # Model ch·∫•t l∆∞·ª£ng cao
            contents=prompt,
            config=GenerateContentConfig(temperature=0.3)
        )
        return response.text.strip()
    except Exception as e:
        print(f"[Gemini Error - Summarization] {e}")
        return "T√≥m t·∫Øt kh√¥ng th√†nh c√¥ng"


# =========================================
# H√†m t√≥m t·∫Øt to√†n b·ªô danh s√°ch b√†i ƒë√£ l·ªçc
# =========================================
def summarize_filtered_papers(filtered_papers):
    """
    T√≥m t·∫Øt abstract c·ªßa t·∫•t c·∫£ c√°c b√†i b√°o ƒë√£ l·ªçc.

    Parameters:
        filtered_papers (list): Danh s√°ch b√†i b√°o ƒë√£ l·ªçc, m·ªói b√†i ch·ª©a 'abstract' v√† 'title'.

    Returns:
        list: Danh s√°ch b√†i b√°o v·ªõi key 'summary' ch·ª©a t√≥m t·∫Øt abstract.
    """
    for paper in filtered_papers:
        abstract = paper.get("abstract", "").strip()
        title = paper.get("title", "Untitled")
        
        if abstract:
            print(f"Summarizing abstract for: {title}")
            paper["summary"] = summarize_with_genai(abstract)
            time.sleep(16)  

    return filtered_papers