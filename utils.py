import os
import glob
import json
from datetime import datetime, timedelta
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from gspread_formatting import (
    CellFormat, Color, TextFormat, format_cell_range, set_column_width
)
from googleapiclient.discovery import build

RESULTS_DIR = "results"
DATABASE_DIR = "database"
DATABASE_FILE = "papers_db.json"
SPREADSHEET_ID = "1ZLTODE7spM_M4mPPy3qeCoR51I_ectk0D5cTiWXWT_k"
creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")


def get_creds():
    creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if creds_path and os.path.exists(creds_path):
        file = creds_path
    else:
        file = r"D:\GitHub\Key_gg_sheet\eternal-dynamo-474316-f6-382e31e4ae72.json"
        if not os.path.exists(file):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y credential: {file}")

    scopes = ["https://www.googleapis.com/auth/spreadsheets"]
    return Credentials.from_service_account_file(file, scopes=scopes)

def normalize_key(paper):
    """
    Chu·∫©n h√≥a key ƒë·ªÉ so s√°nh tr√πng l·∫∑p:
    - ∆Øu ti√™n DOI (lowercase, b·ªè kho·∫£ng tr·∫Øng).
    - N·∫øu kh√¥ng c√≥ DOI ‚Üí d√πng link.
    - N·∫øu kh√¥ng c√≥ link ‚Üí d√πng title.
    """
    doi = (paper.get("doi") or "").strip().lower()
    link = (paper.get("link") or "").strip().lower()
    title = (paper.get("title") or "").strip().lower()

    if doi:
        return doi
    elif link:
        return link
    elif title:
        return title
    return ""

# ==============================
# L·∫•y file JSON m·ªõi nh·∫•t
# ==============================
def get_latest_json():
    """
    L·∫•y file JSON m·ªõi nh·∫•t theo ng√†y c√≥ d·∫°ng: YYYY-MM-DD_allapi_scholar_ndt.json
    """
    pattern = os.path.join(RESULTS_DIR, "*_allapi_scholar_*.json")
    json_files = glob.glob(pattern)

    if not json_files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON n√†o trong th∆∞ m·ª•c results/")
        return None

    # L·∫•y ng√†y t·ª´ t√™n file v√† ch·ªçn ng√†y m·ªõi nh·∫•t
    files_with_dates = []
    for f in json_files:
        base = os.path.basename(f)
        try:
            date_part = base.split("_")[0]  # L·∫•y ph·∫ßn YYYY-MM-DD
            datetime.strptime(date_part, "%Y-%m-%d")  # ki·ªÉm tra format
            files_with_dates.append((date_part, f))
        except Exception:
            continue

    if not files_with_dates:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON h·ª£p l·ªá theo ng√†y")
        return None

    # Ch·ªçn file c√≥ ng√†y m·ªõi nh·∫•t
    latest_file = max(files_with_dates, key=lambda x: x[0])[1]
    print(f"üìÇ File JSON m·ªõi nh·∫•t theo ng√†y: {latest_file}")
    return latest_file


# ==============================
# L∆∞u file JSON v·ªõi timestamp
# ==============================
def save_results_to_json(data, output_dir=RESULTS_DIR, prefix="allapi_scholar_ndt"):
    """
    L∆∞u k·∫øt qu·∫£ v√†o file JSON v·ªõi t√™n ch·ª©a timestamp.
    N·∫øu c√πng 1 ng√†y ƒë√£ c√≥ file -> load d·ªØ li·ªáu c≈©, merge th√™m d·ªØ li·ªáu m·ªõi (l·ªçc tr√πng), r·ªìi ghi ƒë√® l·∫°i.
    """
    os.makedirs(output_dir, exist_ok=True)
    today_str = datetime.now().strftime("%Y-%m-%d")

    # T√¨m file trong ng√†y h√¥m nay
    existing_file = None
    for fname in os.listdir(output_dir):
        if fname.startswith(today_str) and fname.endswith(f"{prefix}.json"):
            existing_file = os.path.join(output_dir, fname)
            break

    merged_data = []
    if existing_file:
        try:
            with open(existing_file, "r", encoding="utf-8") as f:
                old_data = json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file c≈© {existing_file}: {e}")
            old_data = []
        merged_data = old_data
    else:
        # N·∫øu ch∆∞a c√≥ file -> t·∫°o file m·ªõi
        timestamp = datetime.now().strftime("%Y-%m-%d")
        filename = f"{timestamp}_{prefix}.json"
        existing_file = os.path.join(output_dir, filename)

    # Merge d·ªØ li·ªáu (l·ªçc tr√πng theo key chu·∫©n h√≥a)
    existing_keys = {normalize_key(item) for item in merged_data if normalize_key(item)}
    new_filtered = [p for p in data if normalize_key(p) not in existing_keys]

    if not new_filtered:
        print("‚è© Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ th√™m.")
        return existing_file

    merged_data.extend(new_filtered)

    try:
        with open(existing_file, "w", encoding="utf-8") as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        print(f"üíæ ƒê√£ c·∫≠p nh·∫≠t file: {existing_file} (th√™m {len(new_filtered)} b√†i b√°o)")
        return existing_file
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u file JSON: {e}")
        return None


# ==============================
# Load Database DOI
# ==============================
def load_database(db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    os.makedirs(db_dir, exist_ok=True)
    db_path = os.path.join(db_dir, db_file)

    if os.path.exists(db_path):
        try:
            if os.path.getsize(db_path) == 0:
                return []
            with open(db_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è File JSON b·ªã l·ªói ho·∫∑c r·ªóng: {db_path}, t·∫°o database m·ªõi")
            return []
    else:
        return []



def save_database(data, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    os.makedirs(db_dir, exist_ok=True)
    db_path = os.path.join(db_dir, db_file)

    with open(db_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"üíæ Database ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t: {db_path}")

# ==============================
# C·∫≠p nh·∫≠t Database (l∆∞u Title + DOI)
# ==============================
def save_results_to_database(result_file, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    """
    ƒê·ªçc k·∫øt qu·∫£ t·ª´ file JSON v√† l∆∞u v√†o database.
    Chu·∫©n h√≥a key (doi/link/title) v√† lo·∫°i b·ªè tr√πng l·∫∑p.
    """
    if not os.path.exists(result_file):
        print(f"‚ùå File k·∫øt qu·∫£ kh√¥ng t·ªìn t·∫°i: {result_file}")
        return False

    try:
        with open(result_file, "r", encoding="utf-8") as f:
            results = json.load(f)
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc file k·∫øt qu·∫£ {result_file}: {e}")
        return False

    db_data = load_database(db_dir, db_file)
    db_dict = {normalize_key(item): item for item in db_data if normalize_key(item)}

    new_count = 0
    for paper in results:
        key = normalize_key(paper)
        if key and key not in db_dict:
            db_dict[key] = {
                "title": paper.get("title", "Untitled"),
                "doi": paper.get("doi", paper.get("link", ""))
            }
            new_count += 1

    save_database(list(db_dict.values()), db_dir, db_file)
    print(f"‚úÖ ƒê√£ th√™m {new_count} b√†i b√°o m·ªõi v√†o database t·ª´ {result_file}")
    return True



# ==============================
# L·ªçc b√†i b√°o tr√πng 
# ==============================
def filter_duplicates(new_results, results_dir=RESULTS_DIR, db_dir=DATABASE_DIR, db_file=DATABASE_FILE):
    """
    L·ªçc tr√πng c√°c b√†i b√°o m·ªõi b·∫±ng key chu·∫©n h√≥a (doi/link/title).
    - N·∫øu file m·ªõi nh·∫•t l√† h√¥m nay ‚Üí kh√¥ng l·ªçc.
    - N·∫øu file m·ªõi nh·∫•t l√† h√¥m qua ‚Üí l·ªçc theo h√¥m qua.
    - N·∫øu kh√¥ng ph·∫£i h√¥m nay v√† kh√¥ng ph·∫£i h√¥m qua ‚Üí l·ªçc theo database.
    """
    today_str = datetime.now().strftime("%Y-%m-%d")
    yesterday_str = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

    # üîπ L·∫•y file JSON m·ªõi nh·∫•t
    latest_file = get_latest_json()
    if not latest_file:
        return new_results

    # üîπ ƒê·ªçc d·ªØ li·ªáu file m·ªõi nh·∫•t
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            old_results = json.load(f)
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc file {latest_file}: {e}")
        return new_results

    old_dates = {paper.get("pub_date", "") for paper in old_results}

    # ‚úÖ File h√¥m nay ‚Üí kh√¥ng l·ªçc
    if today_str in old_dates:
        print("‚è© File m·ªõi nh·∫•t ƒë√£ l√† h√¥m nay -> Kh√¥ng l·ªçc tr√πng.")
        return new_results

    # ‚úÖ Kh√¥ng ph·∫£i h√¥m nay ‚Üí l·ªçc
    # N·∫øu l√† h√¥m qua ‚Üí l·ªçc theo h√¥m qua
    if yesterday_str in old_dates:
        old_keys = {normalize_key(p) for p in old_results if normalize_key(p)}
        filtered_results = [p for p in new_results if normalize_key(p) not in old_keys]
        removed_count = len(new_results) - len(filtered_results)
        print(f"üóëÔ∏è ƒê√£ lo·∫°i b·ªè {removed_count} b√†i b√°o tr√πng v·ªõi h√¥m qua.")
        return filtered_results

    # ‚úÖ Kh√¥ng ph·∫£i h√¥m qua ‚Üí l·ªçc theo database
    db_path = os.path.join(db_dir, db_file)
    if not os.path.exists(db_path):
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y database -> Tr·∫£ v·ªÅ to√†n b·ªô d·ªØ li·ªáu m·ªõi.")
        return new_results

    try:
        with open(db_path, "r", encoding="utf-8") as f:
            db_data = json.load(f)
            db_keys = {normalize_key(item) for item in db_data if normalize_key(item)}
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc database {db_path}: {e}")
        return new_results

    filtered_results = [p for p in new_results if normalize_key(p) not in db_keys]
    removed_count = len(new_results) - len(filtered_results)
    print(f"üóëÔ∏è ƒê√£ lo·∫°i b·ªè {removed_count} b√†i b√°o tr√πng v·ªõi database.")
    return filtered_results


def tidy_up_sheet_auto(spreadsheet_id, sheet_name=None):
    # 1. K·∫øt n·ªëi Google Sheets API
    creds = get_creds()
    service = build("sheets", "v4", credentials=creds)

    # 2. L·∫•y metadata sheet
    sheet_metadata = service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()
    sheets = sheet_metadata.get("sheets", [])

    if not sheets:
        print("‚ö†Ô∏è File Google Sheets tr·ªëng, kh√¥ng c√≥ sheet n√†o.")
        return

    # 3. X√°c ƒë·ªãnh sheet_id
    sheet_id = None
    if sheet_name:
        for s in sheets:
            if s["properties"]["title"] == sheet_name:
                sheet_id = s["properties"]["sheetId"]
                break
    if not sheet_id:
        sheet_id = sheets[0]["properties"]["sheetId"]
        sheet_name = sheets[0]["properties"]["title"]
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y sheet b·∫°n ch·ªâ ƒë·ªãnh. S·ª≠ d·ª•ng sheet ƒë·∫ßu ti√™n: '{sheet_name}'")

    # 4. L·∫•y d·ªØ li·ªáu hi·ªán t·∫°i ƒë·ªÉ x√°c ƒë·ªãnh s·ªë h√†ng v√† c·ªôt
    result = service.spreadsheets().values().get(
        spreadsheetId=spreadsheet_id, range=sheet_name
    ).execute()
    values = result.get("values", [])
    num_rows = len(values) if values else 1
    num_cols = max(len(row) for row in values) if values else 1

    # 5. T√≠nh chi·ªÅu r·ªông c·ªôt d·ª±a tr√™n n·ªôi dung
    column_widths = [0] * num_cols
    for row in values:
        for col_index in range(num_cols):
            cell_value = row[col_index] if col_index < len(row) else ""
            column_widths[col_index] = max(column_widths[col_index], len(str(cell_value)))

    # 6. T·∫°o danh s√°ch requests
    requests = [
        # CƒÉn gi·ªØa header
        {
            "repeatCell": {
                "range": {
                    "sheetId": sheet_id,
                    "startRowIndex": 0,
                    "endRowIndex": 1
                },
                "cell": {
                    "userEnteredFormat": {
                        "horizontalAlignment": "CENTER",
                        "textFormat": {"bold": True}
                    }
                },
                "fields": "userEnteredFormat(textFormat,horizontalAlignment)"
            }
        },
        # B·∫≠t wrap text to√†n sheet
        {
            "repeatCell": {
                "range": {"sheetId": sheet_id},
                "cell": {"userEnteredFormat": {"wrapStrategy": "WRAP"}},
                "fields": "userEnteredFormat.wrapStrategy"
            }
        },
        # Bo vi·ªÅn to√†n b·∫£ng
        {
            "updateBorders": {
                "range": {
                    "sheetId": sheet_id,
                    "startRowIndex": 0,
                    "endRowIndex": num_rows,
                    "startColumnIndex": 0,
                    "endColumnIndex": num_cols
                },
                "top": {"style": "SOLID", "width": 1},
                "bottom": {"style": "SOLID", "width": 1},
                "left": {"style": "SOLID", "width": 1},
                "right": {"style": "SOLID", "width": 1},
                "innerHorizontal": {"style": "SOLID", "width": 1},
                "innerVertical": {"style": "SOLID", "width": 1}
            }
        }
    ]

    # 7. Resize c·ªôt d·ª±a tr√™n n·ªôi dung
    for col_index, max_len in enumerate(column_widths):
        width_pixels = max(80, min(max_len * 10, 400))  # min 80px, max 400px
        requests.append({
            "updateDimensionProperties": {
                "range": {
                    "sheetId": sheet_id,
                    "dimension": "COLUMNS",
                    "startIndex": col_index,
                    "endIndex": col_index + 1
                },
                "properties": {"pixelSize": width_pixels},
                "fields": "pixelSize"
            }
        })

    # 8. ƒê·∫∑t chi·ªÅu cao t·∫•t c·∫£ h√†ng ~10cm (10cm ‚âà 378px)
    requests.append({
        "updateDimensionProperties": {
            "range": {
                "sheetId": sheet_id,
                "dimension": "ROWS",
                "startIndex": 0,
                "endIndex": num_rows
            },
            "properties": {"pixelSize": 378},
            "fields": "pixelSize"
        }
    })

    # 9. G·ª≠i batchUpdate
    body = {"requests": requests}
    service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=body).execute()

    print(f"‚úÖ ƒê√£ cƒÉn ch·ªânh v√† ƒë·ªãnh d·∫°ng sheet '{sheet_name}' th√†nh c√¥ng!")



def append_json_to_gsheet(df, date_str):
    """Th√™m ho·∫∑c ghi ƒë√® d·ªØ li·ªáu JSON v√†o Google Sheet, kh√¥ng ƒë√® sang ng√†y kh√°c"""
    creds = get_creds()
    client = gspread.authorize(creds)
    sheet = client.open_by_key(SPREADSHEET_ID).sheet1

    all_values = sheet.get_all_values()
    last_col = chr(ord('A') + len(df.columns) - 1)

    # T√¨m xem ng√†y ƒë√£ t·ªìn t·∫°i ch∆∞a
    existing_row = None
    for idx, row in enumerate(all_values):
        if row and row[0].startswith(f"üìÖ Ng√†y {date_str}"):
            existing_row = idx + 1  # gspread index t·ª´ 1
            break

    if existing_row:
        print(f"‚ÑπÔ∏è Ng√†y {date_str} ƒë√£ t·ªìn t·∫°i tr√™n sheet. Ghi ƒë√® d·ªØ li·ªáu.")
        start_row = existing_row

        # T√¨m row cu·ªëi c√πng c·ªßa d·ªØ li·ªáu ng√†y h√¥m nay
        end_row = start_row + 1
        while end_row <= len(all_values):
            if all_values[end_row-1] and all_values[end_row-1][0].startswith("üìÖ Ng√†y"):
                break
            end_row += 1

        # X√≥a to√†n b·ªô v√πng d·ªØ li·ªáu c≈©
        sheet.delete_rows(start_row, end_row - 1)

        next_row = start_row
    else:
        next_row = len(all_values) + 3 if len(all_values) > 0 else 1  # th√™m 2 d√≤ng tr·ªëng

    # üóìÔ∏è Ghi ti√™u ƒë·ªÅ ng√†y
    sheet.update_cell(next_row, 1, f"üìÖ Ng√†y {date_str}")
    try:
        sheet.merge_cells(next_row, 1, next_row, len(df.columns))
    except Exception:
        pass

    # Format ti√™u ƒë·ªÅ ng√†y
    try:
        day_fmt = CellFormat(
            textFormat=TextFormat(bold=True, foregroundColor=Color(1, 0, 0), fontSize=13),
            horizontalAlignment='CENTER',
            backgroundColor=Color(0.85, 0.93, 1.0)
        )
        format_cell_range(sheet, f"A{next_row}:{last_col}{next_row}", day_fmt)
    except Exception:
        pass

    # üß© Ghi header v√† d·ªØ li·ªáu
    data_to_insert = [df.columns.values.tolist()] + df.values.tolist()
    sheet.update(f"A{next_row + 1}", data_to_insert)

    # Format header
    try:
        header_fmt = CellFormat(
            textFormat=TextFormat(bold=True),
            horizontalAlignment='CENTER',
            backgroundColor=Color(0.95, 0.95, 0.95)
        )
        format_cell_range(sheet, f"A{next_row + 1}:{last_col}{next_row + 1}", header_fmt)
    except Exception:
        pass

    # CƒÉn gi·ªØa d·ªØ li·ªáu
    try:
        data_end_row = next_row + len(df) + 1
        center_fmt = CellFormat(
            textFormat=TextFormat(bold=False),
            horizontalAlignment='CENTER'
        )
        format_cell_range(sheet, f"A{next_row + 2}:{last_col}{data_end_row}", center_fmt)
    except Exception:
        pass

    # Auto-width cho c√°c c·ªôt
    try:
        for i in range(1, len(df.columns) + 1):
            set_column_width(sheet, i, 160)
    except Exception:
        pass

    print(f"‚úÖ ƒê√£ th√™m/ghi ƒë√® d·ªØ li·ªáu ng√†y {date_str} v√†o Google Sheet")


def convert_latest_json_to_gsheet():
    """ƒê·ªçc file JSON m·ªõi nh·∫•t (h√¥m nay) v√† n·ªëi v√†o sheet"""
    latest_file = get_latest_json()
    if not latest_file:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON.")
        return

    today_str = datetime.now().strftime("%Y-%m-%d")
    base = os.path.basename(latest_file)
    file_date = base.split("_")[0]

    if file_date != today_str:
        print("‚ÑπÔ∏è File JSON m·ªõi nh·∫•t kh√¥ng ph·∫£i c·ªßa h√¥m nay.")
        return

    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    df = pd.DataFrame(data)
    append_json_to_gsheet(df, today_str)
    tidy_up_sheet_auto(SPREADSHEET_ID)